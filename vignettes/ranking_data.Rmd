---
title: "Ranking data"
author: "Brian J. Knaus"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Ranking data}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---


In the vignette 'Filtering data' we used thresholds to isolate the high quality fraction of variants from a vcf file.
Here we assign ranks to variants within windows.
This information used alone, or in conjunction with thresholds, can also be used to identify high quality variants.


## Data

As in other vignettes, we begin by loading the example data.

```{r}
library(vcfR)
vcf_file <- system.file("extdata", "pinf_sc1_100_sub.vcf.gz", package = "vcfR")
seq_file <- system.file("extdata", "pinf_sc100.fasta", package = "vcfR")
gff_file <- system.file("extdata", "pinf_sc100.gff", package = "vcfR")

vcf <- read.vcf(vcf_file, verbose = FALSE)
dna <- ape::read.dna(seq_file, format = "fasta")
gff <- read.table(gff_file, sep="\t")

chrom <- create_chrom(name="Supercontig_1.100", vcf=vcf, seq=dna, ann=gff, verbose=TRUE)
chrom <- masker(chrom, min_DP = 300, max_DP = 700)
chrom <- proc_chrom(chrom, verbose = TRUE)

```


## Creating scores to rank


Before we can rank our variants, we need to come up with some sort of metric to maximize.
The function masker() is nice because it works on metrics which are commonly available.
The downside is that if your file lacks some of these metrics you have less information to judge the quality.
Also, if a new software generates a metric you really like, you won't be able to use it.
For ranking I've let the user provide a vector of numbers that score each variant.
This provides the flexibility to use what they would like.
They can even combine metrics.
Just keep in mind the scale of your metrics.
For example, if you have a metric that ranges from 0-1 and another which ranges from 1-100 and you add them to create a composite score, the metric that ranges from 1-100 will drive the result more than the one which ranges from 0-1.
If you re-scale the metric which ranges from 0-1, perhaps by multiplying it by 100, you'll equalize its contribution to the final score.
Creating z-scores by subtracting the mean and dividing by the standard deviation would work as well.
In fact, I've created a function to help with that (`?z_score`).


In order to create our vector of scores, let's remind ourselves of what data we have.


```{r}
head(chrom)
```



Let's use the genotype quality and sequence depth from the vcf genotype information.


```{r}
dp <- extract.gt(chrom, element="DP", as.numeric=TRUE)
gq <- extract.gt(chrom, element="GQ", as.numeric=TRUE)
```


The genotype quality is fairly well behaved.
We can look at it for one sample to see its distribution.


```{r, fig.align='center'}
hist(gq[,1])
```


This shows us that for this sample its value is either high, or rather low.
We can use a heatmap to explore this over all samples.


```{r, fig.height=7, fig.width=7}
heatmap.bp(gq)
```


Which validates that this observation extands over all samples.
We can also validate that the values range approximately from 1 to 99 


```{r}
apply(gq, MARGIN=2, range)
```


So let's decide that we're happy with it.


Sequence depth is a more challenging parameter to work with.
In general we don't want low depth.
But high depth could indicate that we're in a repetetive region which we may want to omit.
So we really want to pick an optimal value.
Because each sample is sequenced at a different depth, this optimal value will be different for all of them.
Using z-scores here may work.


```{r}
dpz <- z_score(dp)
dpz <- abs(dpz)
maxes <- apply(dpz, MARGIN=2, max, na.rm=TRUE)
dp2 <- abs(sweep(dpz, MARGIN = 2, STATS = maxes, FUN = "-"))
dp2 <- 100 * sweep(dp2, MARGIN = 2, STATS = maxes, FUN = "/")
```


Creating z-scores centers the data around zero.
By taking the absolule value our optimal value becomes zero and everything high than this is low quality.
But we want to maximize on this value.
When we subtract the maximum values we have values that can be maximized on.
By dividing by the maximum we scale the data from 0-1.
This is converted to 0-100 by multiplying by one hundred.


We can validate that our ranges are from 0-100 with an apply function.


```{r}
apply(dp2, MARGIN=2, range)
```


And visualize with a heatmap.


```{r, fig.height=7, fig.width=7}
heatmap.bp(dp2)
```


And finally, combine our metrics into a composite metric.


```{r, fig.align='center'}
scores <- rowMeans(dp2) + rowMeans(gq)
is.na(scores[c(7,10)]) <- TRUE
```


Check their distribution with a histogram.


```{r, fig.align='center'}
hist(scores)
```


Once we have score in hand we can use them to rank our variants.


```{r}
chrom <- masker(chrom)
chrom <- rank_variants_chrom(chrom, scores)
head(chrom@var.info)
```



This creates a vector of window numbers and rank within each window and adds them to the var.info slot.
We can take a look at them bay calling this directly.


```{r}
cbind(chrom@var.info[1:26, c('POS', 'mask', 'window_number', 'rank')], scores[1:26])
```


We can use this information to create our mask.


```{r}
chrom@var.info$mask[chrom@var.info$rank > 1] <- FALSE
```


And plot.


```{r, fig.height=7, fig.width=7}
chromoqc(chrom, dot.alpha='ff')
```



This looks pretty good.
But there remain variants with a low quality (QUAL).
We can manually adjust this.



```{r}
chrom@var.info$mask[chrom@var.info$MQ < 42] <- FALSE
```


Replot.


```{r, fig.height=7, fig.width=7}
chromoqc(chrom, dot.alpha='ff')
```



## Conclusion


This provides another tool to help filter variant files to the highest quality fraction.
In a previous vignette we used the function masker() to filter the data.
Here we've created a composite score which we'd like to maximize and ranked the variants based on theis score within windows.
A strength of this method is that by using windows we're able to evenly space our variants accross a chromosome.
Choosing the best, or several best, variants per window does not necessarily guaranty high quality variants.
If all of the variants in a window are of low quality then the best of these may still be poor quality.
Some some additional processing may be necessary.
With these tools it is hoped that an individual can rapidly explore their data and determine a method to extract the highest quality variants so that downstream analyses will be of the highest quality possible.









